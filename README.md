This project aims to develop and train a text classification model using the BERT architecture for the specific task of classifying text with the AG_News dataset. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model used for text classification tasks, NER, translation, and many other applications. It is a neural network architecture based on transformers, specifically designed to capture bidirectional context in NLP tasks. The dataset we will use, AG_News, contains news articles from various categories, including sports, business, science and technology, as well as world news. Each article is labeled with the category it belongs to.
